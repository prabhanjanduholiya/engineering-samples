# MongoDB Fundamentals — Developer & Architect Guide

This document explains MongoDB fundamentals from both a developer and an architect perspective. It covers core concepts, data modeling, scaling and operational patterns, consistency and durability controls, security, monitoring, and contains interview questions and model answers suitable for senior developers and architects.

---

## Quick overview
- MongoDB is a distributed document database that stores data as BSON (binary JSON). It is designed for horizontal scale, flexible schemas, rich querying (aggregation pipeline), and operational simplicity.
- Key building blocks: Documents, Collections, Databases, Replica Sets (for HA), and Sharded Clusters (for horizontal scale).

---

## Core concepts (developer view)

- Document: A record expressed as a BSON object. Documents are analogous to JSON and can contain nested objects and arrays.
- Collection: A grouping of documents (similar to a table but schema-free).
- BSON: Binary JSON with additional data types (Date, ObjectId, Binary, Decimal128).
- ObjectId: Default unique identifier generated by MongoDB (contains timestamp, machine id, process id, counter).
- CRUD: Create (insertOne/insertMany), Read (find/findOne/aggregation), Update (updateOne/updateMany/replaceOne), Delete (deleteOne/deleteMany).

Developer tips:
- Favor small, focused documents. Single-document operations are atomic.
- Use projections to return only necessary fields.
- Prefer bulk operations for high-throughput writes (bulkWrite, insertMany with ordered:false when safe).

---

## Data modeling (developer & architect guidance)

Primary design principle: model for your queries. Consider access patterns, latency requirements, expected growth.

Patterns:
- Embed when 1-to-few and data is read/updated together (fast reads, single-doc atomicity).
- Reference when relationships are large, unbounded, or independently updated.
- Bucketing/time-series: group time-series points into fixed-duration documents (hour/day) to avoid very large documents.
- Two-phase or event-sourced patterns: for complex invariants or cross-entity consistency, consider event sourcing or outbox patterns.

Constraints:
- Document size limit (16 MB). Avoid unbounded arrays.
- Keep frequently-updated fields out of large documents when possible.

Shard key considerations (architect role):
- Choose a shard key with high cardinality and even distribution.
- Avoid monotonically increasing keys (ObjectId, timestamps) as sole shard keys to prevent hotspots.
- Consider compound keys (tenantId + userId) or hashed keys for even distribution.

---

## Indexing and query performance

- Index types: single-field, compound, multikey (arrays), text, hashed, TTL, wildcard.
- Use indexes that support your query predicates and sort order. Compound indexes should follow query patterns (left-prefix rule).
- Avoid unbounded index growth by pruning unused indexes. Each index increases write cost.
- Use explain() to study query plans and identify COLLSCAN, IXSCAN, or covered queries.
- Covering indexes can eliminate document fetches when projection fields are included in the index.

Performance tips:
- Monitor index usage and cardinality; highly selective indexes help queries.
- Avoid large multikey indexes on high-cardinality arrays.

---

## Aggregation framework

- Aggregation pipelines allow complex transformations and analytics server-side (match, project, group, sort, lookup, facet, unwind, bucket, graphLookup).
- Pipelines can be optimized with $match and $sort early and by using indexes for pipeline stages that can be covered.
- Use $lookup for server-side joins when necessary; prefer denormalization when joins are frequent and latency-sensitive.

---

## Transactions and concurrency

- MongoDB supports multi-document transactions in replica sets and sharded clusters (since 4.0+ for replica sets, 4.2+ for sharded clusters).
- Transactions provide ACID guarantees but add latency and reduce concurrency. Use them when strong cross-document consistency is required.
- For many workloads, design to use single-document atomic operations and idempotent updates instead of transactions.
- Use optimistic concurrency with findAndModify, update with query predicates (e.g., version fields), or retryable writes.

---

## Replication, High Availability, and Consistency (architect view)

- Replica set: a group of mongod instances that maintain the same dataset. One primary accepts writes; secondaries replicate and can serve reads if configured.
- Election: when the primary fails, secondaries elect a new primary using replica set configuration and heartbeat/priority rules.
- Read Concern: control the visibility of data on reads (local, available, majority, linearizable). "majority" provides stronger guarantees for seeing acknowledged writes.
- Write Concern: control durability acknowledgement (w:1, w:majority, w:0). Higher write concern increases durability at cost of latency and availability under partitions.
- Read Preference: control how reads are routed (primary, primaryPreferred, secondary, secondaryPreferred, nearest). Use appropriate read preference based on consistency needs.

Architect tips:
- For critical writes, use writeConcern: "majority" and readConcern: "majority" when you need linearizable-like behavior.
- For global deployments, prefer zoned clusters or replica sets per region and application-level routing to minimize cross-region latency and meet latency/consistency tradeoffs.

---

## Sharding and horizontal scaling (architect view)

- Sharded cluster components: mongos (query router), config servers (metadata, typically replica set of 3), and shards (each shard is usually a replica set).
- Shard key selection is the most important scaling decision. Good shard keys spread CRUD load and shard key is included in query predicates for targeted operations.
- Chunk migrations: MongoDB splits data into chunks and balances chunks across shards; balancer moves chunks online.
- Resharding: newer MongoDB versions support online resharding; still a significant operational task and should be planned carefully.

Sharding tips:
- Use hashed shard keys to spread simple point traffic, or compound keys to support targeted range queries and distribution.
- Avoid shard key patterns that cause scatter-gather queries (queries that don't include the shard key) to reduce cross-shard traffic.
- Monitor chunk distribution and balancer activity; throttle migrations in heavy-load windows.

---

## Storage engines and performance

- WiredTiger is the default storage engine for MongoDB: document-level concurrency, compression (snappy/zlib/zstd), and an LRU cache for memory management.
- RocksDB (via third-party builds) is used where low-level storage optimization is needed but isn't standard in upstream MongoDB.
- Tuning: allocate appropriate wiredTiger cache size (by default ~50% of RAM minus 1 GB), provision fast disks (NVMe/SSD), and monitor page faults.

---

## Change streams and triggers

- Change Streams allow applications to subscribe to realtime data changes (insert/update/delete/replace) from replica sets or sharded clusters.
- Useful for caches invalidation, event-driven architectures, search indexing, analytics pipelines, and outbox pattern consumption.
- Change streams can be resumed using resume tokens to provide fault-tolerant event processing.

---

## Backup, restore, and disaster recovery

- Backup strategies: filesystem snapshots, mongodump/mongorestore (logical backup), and cloud provider snapshots or MongoDB Ops Manager/Atlas snapshots (physical snapshots).
- Point-in-time recovery: use oplog tailing to replay operations to a specific time when supported.
- Test restore procedures regularly; ensure backups are stored off-cluster and validated.

---

## Security and multi-tenancy

- Authentication: SCRAM-SHA-1/SCRAM-SHA-256, x.509 certificates, LDAP/AD integration.
- Authorization: role-based access control (RBAC) with built-in and custom roles.
- Network security: enable TLS for client-server and inter-node traffic, VPC/Private networking, firewall rules.
- Encryption: at-rest encryption (storage encryption) and field-level encryption for sensitive data.
- Audit logging: capture database activity for compliance.

Multi-tenant patterns:
- Database per tenant: strong isolation but limited scalability for many tenants.
- Collection per tenant: improved density but more management overhead.
- Shared collection with tenantId field: best density and flexible sharding; ensure logical isolation and row-level authorization.

---

## Monitoring and observability

- Metrics to track: operation latencies, QPS, connections, resident memory, cache/WT cache miss ratio, index miss ratio, replication lag, lock percentage, journal commit time, disk I/O.
- Tools: MongoDB Cloud/Atlas monitoring, MMS/Ops Manager, Prometheus exporters, Grafana dashboards, and built-in serverStatus and profiler.
- Use slow query profiler and explain to identify problematic queries and missing indexes.

---

## Operational best practices (architect)

- Capacity planning: size for working set in RAM (indexes + hot data), plan for headroom for rebalancing and compaction.
- Routine maintenance: monitor compaction, oplog size, disk usage, and index fragmentation.
- Upgrades: perform staged rolling upgrades for replica sets; test upgrades in staging.
- Security hardening: least privilege, encrypted transport, and regular vulnerability scans.

---

## Common pitfalls and mitigations

- Hotspots from poor shard key choice: mitigate with hashed keys or resharding.
- Large documents/unbounded arrays: mitigate with bucketing or child collections.
- Excess indexes harming write throughput: minimize indexes to those required and use compound indexes where appropriate.
- Cross-shard transactions and scatter-gather queries: redesign schema or ensure shard key is present in queries.

---

## Interview questions & model answers (Senior Developer / Architect)

Note: answers are concise model responses. Interviewers will expect follow-ups about trade-offs and design specifics.

1) Why choose MongoDB for a new product? What workloads suit it best?
Answer: MongoDB suits document-oriented workloads with evolving schemas, hierarchical data, and when you need flexible queries and secondary indexes. Good for content management, catalogs, user profiles, operational analytics, and rapid development. Choose it when single-document atomicity, flexible schema, and horizontal scaling are important.

2) How does MongoDB ensure high availability and what happens if the primary fails?
Answer: Replica sets provide HA. Secondaries replicate the primary's oplog. If primary fails, remaining members hold an election (using heartbeats and votes) to pick a new primary. Election behavior can be tuned with priorities, votes, and tags.

3) How do readConcern and writeConcern affect consistency and durability?
Answer: writeConcern controls replication acknowledgement (w:1, w:majority). readConcern controls visibility of committed data on reads (local, majority). Using w:majority with readConcern:majority gives stronger guarantees that reads will see writes acknowledged by a majority.

4) How would you pick a shard key for a multi-tenant SaaS application?
Answer: Use tenantId as part of the shard key to isolate tenant data and route tenant traffic to a small subset of shards. Combine tenantId with another field (e.g., userId or hashed value) to avoid cases where one tenant dominates and to improve distribution.

5) Describe how to migrate a growing collection to a sharded cluster.
Answer: Plan: pick shard key, provision sharded cluster (config servers, mongos, shards), import data (mongoimport/mongorestore or live resharding), enable sharding on DB and collection, create necessary indexes (including shard key prefix), and then let balancer migrate chunks. Monitor balancer and replication lag and throttle migrations if needed.

6) When do you use transactions vs single-document operations?
Answer: Use transactions for multi-document atomicity when invariants span documents/collections and correctness is critical. Prefer single-document operations for performance; design schema to reduce need for multi-document transactions.

7) How to handle offline nodes and eventual consistency in reads?
Answer: Use appropriate readPreference and readConcern. For stale-tolerant reads, read from secondaries. To ensure fresh data despite offline nodes, use w:majority and readConcern:majority or route critical reads to primary.

8) How do you design for high write throughput in MongoDB?
Answer: Shard using a key that spreads writes evenly, use bulk/batched writes, minimize indexes, provision fast storage, tune WiredTiger cache, and tune writeConcern for acceptable durability. Use journaling and commit settings appropriately.

9) Explain how change streams can be used in an event-driven architecture.
Answer: Change streams expose a reliable, resumable feed of DB changes. Consumers can subscribe to changes, process them (update caches, push to search index, emit domain events), and use resume tokens to restart after failures.

10) What are common causes of replica set elections and how do you mitigate churn?
Answer: Causes: primary crash, network partitions, long GC/stop-the-world pauses, heavy replication lag. Mitigate by stable hardware, tuning timeouts (heartbeatIntervalMillis), setting appropriate election priorities and votes, and ensuring secondaries can keep up with oplog (sufficient IO and network).

11) How do you secure a MongoDB deployment in production?
Answer: Enable authentication (SCRAM-SHA-256), enforce TLS for all connections, enable authorization with least-privilege roles, use encrypted storage, restrict network access with VPC/Firewalls, and enable audit logging.

12) How would you perform backups and enable point-in-time recovery?
Answer: Use snapshots for physical backups (consistent filesystem snapshots), or Ops Manager/Atlas snapshotting. For PITR, ensure oplog is retained long enough and use oplog tailing to replay operations to a target time.

13) What metrics do you monitor for MongoDB health?
Answer: oplog window and replication lag, operation latencies, query performance (slow queries), connections, memory/RSS, WiredTiger cache usage and page faulting, disk I/O, lock percentage, and journal commit time.

14) Explain resharding and when to use it.
Answer: Resharding changes a collection's shard key and redistributes data. Use resharding when the original shard key leads to hotspots/imbalance or query inefficiencies. It should be planned and monitored because it moves significant data.

15) How do you optimize aggregation pipelines?
Answer: Push $match and $sort early, use $project to limit fields, create supporting indexes for $match/$sort, avoid expensive $lookup on very large datasets (or pre-aggregate/denormalize), and use $facet for parallel sub-pipelines where appropriate.

16) How would you design schema for timeline feeds (social network) with high fan-out?
Answer: Options: fan-out-on-write (materialize feed per follower) for fast reads but expensive writes; fan-out-on-read (compute feed on demand) for cheaper writes but heavier read-time compute. Hybrid: precompute for most users but compute on-demand for celebrities. Use denormalized cache and TTL for ephemeral data.

17) How does WiredTiger impact concurrency compared to MMAPv1?
Answer: WiredTiger provides document-level concurrency and better throughput for concurrent workloads. It uses compression and an internal cache; tuning cache size and compaction behavior affects performance.

18) How to avoid hot primary or secondary in cloud regions?
Answer: Use zone awareness, tag-aware sharding/replica placement, or provision regional replica sets. Use read preference to route reads to local secondaries and route writes to local primaries (or use multi-region architecture with application-level routing).

19) What is the oplog and why is its size important?
Answer: Oplog is a capped collection storing replication operations. Its size determines the oplog window (how far back secondaries can catch up). If a secondary falls behind longer than the oplog window, it must resync from a full snapshot.

20) How would you approach capacity planning for a MongoDB cluster?
Answer: Estimate working set (indexes + hot data) to fit into RAM for low latency. Plan for growth, rebalancing overhead, and ops like compaction. Size disks and IOPS for peak write/read load and ensure sufficient network bandwidth.

---

## Quick reference: common commands (developer)

- Start a replica set member: mongod --replSet rs0 --dbpath /data/db --port 27017
- Initialize a replica set (mongo shell): rs.initiate()
- Add a member: rs.add("host:port")
- Check replication status: rs.status()
- Enable sharding on a DB: sh.enableSharding("mydb")
- Shard a collection: sh.shardCollection("mydb.mycoll", { shardKey: 1 })
- Show indexes: db.collection.getIndexes()
- Explain a query: db.collection.find({...}).explain("executionStats")

---

## Final advice
- Start with realistic data and query patterns. Prototype and measure.
- Prefer schema designs that avoid cross-shard operations and minimize multi-document transactions for scale.
- Invest in monitoring, backup/restore tests, and security before production traffic.
- For large-scale or global workloads, evaluate managed offerings (MongoDB Atlas, cloud-managed services) to reduce operational burden.

---

Appendix: Further reading
- Official MongoDB manual: https://docs.mongodb.com/
- MongoDB Schema Design Patterns
- MongoDB Production Notes and Architecture Guides


